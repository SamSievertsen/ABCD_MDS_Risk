---
title: "Proposal for Utilizing ARC High-Performance Computing Resource"
author: "Sam A. Sievertsen"
date: "`r Sys.Date()`"
output:
  html_document:
    self_contained: true
---

```{r global, include = FALSE}

# Set global env variables
knitr::opts_chunk$set(warning = FALSE, message = NA, comment = "")

```

## **Introduction**

Hello Dr. Nagel! Sam Sievertsen in Dr. Huber's lab here. As part of my first year project (FYP), I'd like to humbly request access to ARC High-Performance Computing (HPC) resources under your existing group allocation for the purpose of validating cluster solutions on the ABCD Release 5.1 dataset. These analyses require significant computational resources beyond my local computing capacity. Utilizing ARC would substantially improve my analysis efficiency, and hopefully have minimal impact on your resources. 

In the event it is helpful data, I've outlined the specifics of this process below, and what I expect it to entail logistically. If I can provide any additional information to be helpful, and/or an alternative solution would be preferable, I would be happy to go another suggested route. Thank you in advance for your time! 

### **TL;DR**

I am asking to please be granted temporary access to the ARC High-Performance Computing cluster under your existing group to run computationally intensive validation analyses on the ABCD Release 5.1 dataset (likely already stored in your storage). These analyses involve evaluating multiple clustering solutions (~704 parameter combinations per validation method), which are impractical to run locally. Using ARC’s "basic access" nodes (available at no cost) or your existing allocation (~$58 total if basic nodes are unavailable or not feasible) would dramatically improve efficiency, reducing runtime from multiple days necessitating software to keep my laptop open to less than an hour per analysis. If approved, I believe the action needed from you would be adding my user account under your ARC group. I will hopefully be able to handle everything else, including data verification, analyses execution, and prompt storage cleanup.

## **Specifics of the Request**

### **Dataset and Storage**

- **Data I am planning to use:** ABCD Release 5.1
- **Data location:**  
  - If the release 5.1 dataset already resides within your ARC storage allocation, no additional storage or upload is necessary.
  - If the dataset is unavailable, temporary storage (~1.59 GiB maximum) will be required.
- **Output storage:** Approximately **10MB per validation run**.  
  *(Actual example RDS file: ~9MB)*

### **Analyses Needing HPC (In Chronological Order)**

The following are analyses I have planned, which would likely only be possible with resources available from an HPC such as the ARC (specifically, for optimizing cluster solutions):

1. **Optimal Lambda (Distance Parameter) Estimation** (k-prototypes algorithm)
2. **Distance Method Comparison** (Huang vs. Gower)
3. **Computation of Validation Indices** (`clustMixType::validation_kproto()`):
    - **C Index**
    - **Point-biserial**
    - **Silhouette Index** *(partially completed)*
    - **Tau**
    - **Gamma**

Each validation index requires assessing approximately **704 possible combinations** of the aforementioned variables.

### **Current Local Runtime Analysis**

Below are runtime estimates based on just the initial silhouette (1 out of 5 considered) validation analyses conducted locally (Windows Machine, Intel i5 14th gen, 16 GiB RAM) for each of the 704 validation combinations:

```{r runtime-data, echo=FALSE, message=FALSE, warning=FALSE}

library(dplyr)
library(ggplot2)

# Load actual runtime data provided by user
runtime_data <- read.csv("../../results/main_analysis/k_optimization_runtime_log.csv")

# Summarize runtime statistics
runtime_summary <- runtime_data %>%
  summarize(
    mean_RuntimeSeconds = mean(RuntimeSeconds),
    median_RuntimeSeconds = median(RuntimeSeconds),
    min_RuntimeSeconds = min(RuntimeSeconds),
    max_RuntimeSeconds = max(RuntimeSeconds))

knitr::kable(runtime_summary, caption = "Summary of Local Validation Runtime (Seconds per Solution)", col.names = c("Mean Run Time (sec)", "Median Run Time (sec)", "Min Run Time (sec)", "Max Run Time (sec)"), digits = 2, align = "l")

```

**Local projected total runtime** (all 704 solutions):  

Approximately `r round(runtime_summary$mean_RuntimeSeconds * 704 / 3600, 2)` hours based on average runtime per solution, and approximately `r round((runtime_summary$mean_RuntimeSeconds * 704 / 3600)*5, 2)` hours total across all validation indices. Given that my machine locks after 15 minutes due to OHSU requirements and the total run time for all indices likely being a lot higher due to exponential complexity/computational load for some of the indices, this amount of runtime is likely not feasible on my local machine. This is further compunded considering the likely scenario in which I do not get the algorithm right on the first run. 

### **Estimated HPC Runtime and Costs**

Given ARC resources (up to **36 CPUs and 256GB RAM** per "basic access" node), the following is the estimated run time parameters:

- **Estimated parallel runtime:**  
  - Runtime per solution: average ~`r round(runtime_summary$mean_RuntimeSeconds, 2)` sec  
  - Total serial runtime: ~`r round(runtime_summary$mean_RuntimeSeconds * 704 / 3600, 2)` hours  
  - Using 36 CPUs in parallel: estimated HPC runtime ~`r round((runtime_summary$mean_RuntimeSeconds * 704) / (3600 * 36), 2)` hours total (less than 1 hour per validation index).

- **Cost estimation (if basic access unavailable):**
  - ARC rate: **$0.025 USD per CPU-hour**  
  - Total CPU-hours estimated per validation index:
    - Serial hours per index: ~`r round(runtime_summary$mean_RuntimeSeconds * 704 / 3600, 2)`  
    - Parallelized CPU-hours: Same as serial total (since parallel distribution across CPUs does not change total CPU-hour consumption)
    - **Estimated cost per validation index**:  
      `r round(runtime_summary$mean_RuntimeSeconds * 704 / 3600, 2)` hours × 36 CPUs × $0.025/hr =  
      **$`r round(runtime_summary$mean_RuntimeSeconds * 704 / 3600 * 36 * 0.025, 2)` USD** per validation index  
  - For 5 indices: ~ **$`r round(runtime_summary$mean_RuntimeSeconds * 704 / 3600 * 36 * 0.025 * 5, 2)` USD** total (if not using basic nodes).

- **Cost with Basic Access**: **$0 USD** (8 free basic runs available).

### **Proposed Workflow Steps Should you Approve**

Upon approval, I would:

1. Verify ABCD 5.1 data availability on your ARC storage.
2. Setup user account under your ARC group.
3. Execute validation analyses using basic access nodes (if available), minimizing cost.
4. Promptly transfer and clear output files (~10MB per run).

### **Request for HPC Resource Access**

Given the information above, I humbly request your approval to proceed with analyses using your ARC allocation, assuming that resource usage is minimal and no or low-cost. I greatly appreciate your consideration and any support you can offer in facilitating this stage of my project.

**All the best,**  
*Sam Sievertsen*  
*First Year Clinical Psychology PhD Student, MIND Lab*  
*03/20/2025*
