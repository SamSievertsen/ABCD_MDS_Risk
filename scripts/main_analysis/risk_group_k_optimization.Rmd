---
title: "Mood Disorder & Suicidality Risk Group Clustering k Validation + Optimization"
author: "Sam A. Sievertsen"
date: "`r Sys.Date()`"
output:
  html_document:
    self_contained: true
---

```{r global, include = FALSE}

# Set global env variables
knitr::opts_chunk$set(warning = FALSE, message = NA, comment = "")

```

```{r environment, echo = FALSE, include = FALSE, warning = FALSE}

# Load necessary packages + environmental variables
library(knitr)
library(dplyr)
library(tidyr)
library(DescTools)
library(skimr)
library(easystats)
library(cluster)
library(clustMixType)
library(factoextra)
library(NbClust)
library(clValid) 
library(ClusterR) 
library(clusterSim)
library(clustertend)
library(hopkins)
library(tidymodels)
library(tidyclust)
library(tidyquant)
library(ggplot2)
options(scipen = 999, digits = 8)

# Read in risk variable data to be used in clustering
risk_variable_data <- read.csv("../../data/data_processed/risk_variable_data.csv")

```

## Data Prep for Analysis 

```{r data prep, warning = FALSE}

## Data Prep ## 

#1. Z-score continuous data for clustering
#1.1 Create a list of continuous variables to z-score
variables_to_scale <- c(
  "cbcl_scr_dsm5_depress_t",
  "cbcl_scr_dsm5_anxdisord_t",
  "cbcl_scr_syn_attention_t",
  "cbcl_scr_syn_aggressive_t",
  "pgbi_p_ss_score",
  "upps_y_ss_negative_urgency",
  "upps_y_ss_positive_urgency",
  "sds_p_ss_total",
  "nsc_p_ss_mean_3_items",
  "nihtbx_list_uncorrected",
  "nihtbx_flanker_uncorrected",
  "nihtbx_pattern_uncorrected",
  "ACE_index_sum_score")

#1.2 Z-score all continuous variables to be used in clustering
risk_variable_data <- risk_variable_data %>%
  mutate(across(
    all_of(variables_to_scale),
    ~ as.numeric(scale(.)),
    .names = "{.col}_scaled"))


#2. Ensure dichotimization of categorical data for clustering
risk_variable_data <- risk_variable_data %>% 
  mutate(across(c("family_history_depression", "famhx_ss_momdad_ma_p", "bullying"), as.factor))

#3. Retain only columns of interest for clustering
risk_variable_data <- risk_variable_data %>% 
  dplyr::select(c(src_subject_id, eventname, family_id, site_name, sex, age_in_years, race_ethnicity, family_history_depression, famhx_ss_momdad_ma_p, bullying, cbcl_scr_dsm5_depress_t_scaled, cbcl_scr_dsm5_anxdisord_t_scaled, cbcl_scr_syn_attention_t_scaled, cbcl_scr_syn_aggressive_t_scaled, pgbi_p_ss_score_scaled, upps_y_ss_negative_urgency_scaled, upps_y_ss_positive_urgency_scaled, sds_p_ss_total_scaled, reshist_addr1_coi_z_coi_nat, nsc_p_ss_mean_3_items_scaled, nihtbx_list_uncorrected_scaled, nihtbx_flanker_uncorrected_scaled, nihtbx_pattern_uncorrected_scaled, ACE_index_sum_score_scaled))

#4. Retain subject IDs as row names while removing variables that will not be clustered 
#4.1 Remove all variables except subject ID from the dataframe for clustering
risk_variable_clustering_data <- risk_variable_data %>%
  dplyr::select(-eventname, -family_id, -site_name, -sex, -age_in_years, -race_ethnicity)

#4.2 Set subject IDs as row names
row.names(risk_variable_clustering_data) <- risk_variable_clustering_data$src_subject_id

#4.3 Remove 'src_subject_id' column from data
risk_variable_clustering_data <- risk_variable_clustering_data %>%
  dplyr::select(-src_subject_id)

```

## Determining Optimal Number of Clusters

### Generating Cluster Validation Indices

In order to minimize run time, the following validation indices will be used to determine the optimal k (as opposed to utilizing all available metrics): 

- **C Index**: a normalized sum of the distances between all the pairs of objects that belong to the same cluster; the normalization scheme, which is based on the minimum *Smin* and maximum *Smax* [distance sums in the dataset](https://www.sciencedirect.com/science/article/pii/S0169743924000571)
    - is easy to compute and interpret, fast and efficient in terms of computational time, does not require any prior knowledge about the data or the number of clusters, works well with datasets of various shapes and sizes, and [can handle outliers well](https://www.sciencedirect.com/science/article/pii/S092523122400969X)
- **Point-biserial**: represents the point-biserial [correlation coefficient](https://www.sciencedirect.com/topics/physics-and-astronomy/correlation-coefficient) between the pairwise distance matrix and a binary matrix consisting of 0/1 entries that indicate whether or not two objects are in the [same cluster](https://www.sciencedirect.com/science/article/pii/S0169743924000571#bib10), [has good sensitivity](https://www.sciencedirect.com/science/article/pii/S0169743924000571#sec4), and tends to both run quickly and [estimate the correct number of clusters in k-prototypes](https://doi.org/10.5445/KSP/1000098011/02)
- **Silhouette Index**: defines a cluster membership score for each object. The membership score is calculated by comparing the distance of each object from the centroid of its cluster and its [minimum distance from the other cluster centroids](https://www.sciencedirect.com/science/article/pii/S0169743924000571)
    - is [“an unambiguous optimal point determination”](https://www.sciencedirect.com/science/article/pii/S0169743924000571)
- **Tau**: Kendall's rank [correlation coefficient](https://www.sciencedirect.com/topics/earth-and-planetary-sciences/correlation-coefficient) between the ranks, which are corrected for ties, assigned to the object pairs on the basis of their proximity, where similar object pairs are assigned the lower ranks, and the [binary vector in which a value of 0 is assigned to a pair of objects that belong to the same cluster and a value of 1 to a pair of objects that belong to different clusters](https://www.sciencedirect.com/science/article/pii/S0169743924000571#sec4)
    - The computational formula of Tau index is where the term *t* in the denominator indicates the number of comparisons of two pairs of objects such that both pairs represent within cluster comparisons (i.e., within-cluster distances) or both pairs [arc between cluster comparisons (i.e., between-cluster distances)](https://www.sciencedirect.com/science/article/pii/S0169743924000571#sec4)
    - also performs very well on k-prototypes [(but has highest run time)](https://doi.org/10.5445/KSP/1000098011/02)
- **Gamma**: an adaptation of [Goodman and Kruskal's Gamma correlation index](https://www.tandfonline.com/doi/abs/10.1080/01621459.1975.10480256) to be used for [clustering applications](https://link.springer.com/article/10.1007/bf02294245); this is another measure of rank correlation whose maximum value 1 is obtained if there is no pair of objects in the same cluster, which is less similar than a pair of objects in different clusters
    - [Also performs well](https://www.sciencedirect.com/science/article/pii/S0169743924000571#sec4)

Based on initial benchmarking, parallelization of CPU cores (on local machine "Dell Latitude 5550"; HPC benchmarking not completed) actually results in decreased performance and 63.73% longer runtime. As such, iterative generation of validation metrics below will not be parallelized. 

Moreover, based on test data (*n* = 100; same distributions as actual dataset), initial estimated local machine runtime for this procedure was ~ 7.52 hours. However, something that I did not account for is that most of the index calculations for clustering involves pairwise distances between each datapoint, requiring over 41.9 million computations for a dataset of 9,153 subjects, significantly more than for smaller datasets (e.g.,only ~4,950 pairs for \(n = 100\)). Testing multiple \(k\)-values (3:12) as we are also multiplies this cost, as each index is recomputed for each \(k\). Additionally, setting `nstart = 8` increases demands by running the clustering algorithm 8 times per \(k\) for robust results. As such, run time for increasing dataset n becomes lengthier by orders of magnitude, and not linearly. 

Because of this, just the first validation index in the list of indices to assess (C Index) using the first distance method (Huang) alone took 26 hrs 30 mins to complete. Though the computational power of HPC may be able to drastically reduce this runtime and eliminate the burden of having to keep my local machine on and running for > 265 hours for all 10 combinations of distance metric + validation index, it will not be feasible for initial clustering to generate all of these validation indices. 

As such, given that the silhouette index is [“an unambiguous optimal point determination”](https://www.sciencedirect.com/science/article/pii/S0169743924000571) method, performance of each distance method + optimal k will be assessed using primarily the silhouette method for now (and potentially the point-biserial index [given its relatively quicker run time than other indices](https://doi.org/10.5445/KSP/1000098011/02)), followed by elbow plot examination and evaluation of additional k cluster performance. 

Additionally, given the tendency of initial versions of the clustering algorithm to over-weight the separation of the clusters based on the categorical variables, we made the following changes to this iteration of the script relative to the previous for optimizing the k value, distance method, and lambda parameters for the current studies clustering: 

1. ***Extended Lambda Testing**: 

  - Three new methods were used in comparison to the default package method: 
  
    - Averaged by variable type lambda values: Use the averaged by variable type (i.e., categorical and continuous) variance derived lambda values (i.e., ~0.2 for categorical variables and 1.0 for continuous variables)
    
      - According [to Hennig & Liao (2013)](https://rss.onlinelibrary.wiley.com/doi/epdf/10.1111/j.1467-9876.2012.01066.x), appropriate weighting of variables (e.g., continuous vs. categorical) is crucial to avoid dominance by variables with inherently smaller or larger scales. Averaging lambda values by variable type could therefore provide proportional contributions
    
    - Normalized lambda values: 
    
      - Same empirical justification as above; this option provides similar, potentially more nuanced information for each variable
    
    - Grid search-derived lambda values: iteratively evaluates lambda values to determine the optimal lambda for k-prototypes clustering. The `optimize_lambda` function tests each lambda using significance counts from statistical tests for continuous and categorical variables.
      
      - [Van de Velden et al. (2023)](https://arxiv.org/pdf/2301.02190) emphasize the importance of tailored distance measures for clustering, especially for mixed data types. Using grid search enables customization of lambda for improved clustering performance by aligning it with the data structure
    
      - Multiple lambda values tied with the highest score during the grid search are iteratively tested

2. **Silhouette Score Methodology**:

  - Replaced computationally intensive validation methods with `cluster::silhouette` and `cluster::daisy` for faster runtime
  
  - Output was previously found to be equal given that both the `cluster` package methods and `kproto_validation` use a dissimilarity/distance matrix; the `cluster` package is just exponentially more optimized

```{r k validation indices, warning = FALSE}

## Generate Validation Indices to be Used in Determining Optimal k ##

#1. Define parameters of k clustering + optimization
#1.1 Define k range of clusters to test
k_range <- 2:12

#1.2 Define distance methods to be used during optimization
distance_types <- c("huang", "gower")

#1.3 Define lambda testing procedures
#1.31 Averaged lambda by variable type
#1.311 Create a vector containing the categorical variable names
categorical_vars <- c("family_history_depression", "famhx_ss_momdad_ma_p", 
                      "bullying")

#1.312 Create a vector containing the continuous variable names
continuous_vars <- c("cbcl_scr_dsm5_depress_t_scaled", "cbcl_scr_dsm5_anxdisord_t_scaled",
                     "cbcl_scr_syn_attention_t_scaled", "cbcl_scr_syn_aggressive_t_scaled",
                     "pgbi_p_ss_score_scaled", "upps_y_ss_negative_urgency_scaled",
                     "upps_y_ss_positive_urgency_scaled", "sds_p_ss_total_scaled",
                     "reshist_addr1_coi_z_coi_nat", "nsc_p_ss_mean_3_items_scaled",
                     "nihtbx_list_uncorrected_scaled", "nihtbx_flanker_uncorrected_scaled",
                     "nihtbx_pattern_uncorrected_scaled", "ACE_index_sum_score_scaled")

#1.313 Determine the averaged by variable type lambda values
lambdaest(risk_variable_clustering_data, verbose = TRUE, outtype = "variation", num.method = 2, fac.method = 2)

#1.314 Create a vector containing the averaged by variable type lambda values
lambda_averaged <- c(rep(0.20, length(categorical_vars)), rep(0.93, length(continuous_vars)))

#1.32 Normalized lambda values
lambda_raw <- lambdaest(risk_variable_clustering_data, verbose = TRUE, outtype = "variation", num.method = 2, fac.method = 2)
lambda_normalized <- lambda_raw / mean(lambda_raw)

#1.33 Grid search lambda values
lambda_grid <- seq(0.1, 5, by = 0.1)

#2. Perform clustering optimization
#2.1 Define the lambda grid search optimization function
optimize_lambda <- function(data, lambda) {
  
  #2.11 Perform 5-group clustering
  model <- kproto(data, k = 5, lambda = lambda, nstart = 8, verbose = FALSE)
  data$cluster <- model$cluster
  
  #2.12 Initialize significance counter for each lambda iteration
  sig_count <- 0
  
  #2.13 Continuous variables: Difference in means (t-test) - p values < FDR corrected alpha = sig
  for (var in continuous_vars) {
    t_test <- data %>%
      t_test(as.formula(paste(var, "~ as.factor(cluster)")), detailed = TRUE)
    if (t_test$p_value < 0.0029) sig_count <- sig_count + 1
  }
  
  #2.14 Categorical variables: Difference in proportions (chi-square test) - p values < FDR corrected alpha = sig
  for (var in categorical_vars) {
    chi_test <- data %>%
      dplyr::select(cluster, !!sym(var)) %>%
      mutate(cluster = as.factor(cluster)) %>%
      chisq_test(as.formula(paste("`", var, "` ~ cluster", sep = "")))
    if (chi_test$p_value < 0.0029)
      sig_count <- sig_count + 1
  }
  
  #2.15 Return the total count of significant variables
  return(sig_count)
}

#2.2 Perform Lambda Grid Search
#2.21 Create an empty dataframe to store grid search results
lambda_grid_results <- data.frame(lambda = lambda_grid, score = NA)

#2.22 Iterate through all values in the lambda grid and store results
for (i in seq_along(lambda_grid)) {
  lambda_grid_results$score[i] <- optimize_lambda(risk_variable_clustering_data, lambda_grid[i])
}

#2.23 Identify all optimal lambda values with the highest score
max_grid_search_score <- max(lambda_grid_results$score, na.rm = TRUE)
optimal_lambda_grid <- lambda_grid_results$lambda[lambda_grid_results$score == max_grid_search_score]

#2.3 Perform k, distance, and lambda optimization
#2.31 Initialize placeholders for k optimization
#2.311 Initialize lambda configurations
lambda_types <- list(
  "averaged" = lambda_averaged,
  "normalized" = lambda_normalized,
  "default" = NULL
)

#2.312 Add each optimal lambda from the grid search to the lambda configurations
for (i in seq_along(optimal_lambda_grid)) {
  lambda_types[[paste0("grid_search_", i)]] <- optimal_lambda_grid[i]
}

#2.313 Placeholder for k optimization index results
k_optimization_indices <- list()

#2.314 Create a dataframe to track individual validation index runtimes
runtime_log <- data.frame(
  method = character(),
  distance = character(),
  lambda_type = character(),
  RuntimeSeconds = numeric(),
  stringsAsFactors = FALSE
)

#2.315 Placeholder for silhouette scores
silhouette_scores <- data.frame(
  k = integer(),
  lambda_type = character(),
  distance = character(),
  silhouette_score = numeric()
)

#2.32 Perform k clustering optimization + silhouette calculation
for (lambda_name in names(lambda_types)) {
  lambda_values <- lambda_types[[lambda_name]]
  
  for (distance in distance_types) {
    
    for (k in k_range) {
      
      #2.321 Print clustering progress
      cat("[", paste0(Sys.time()), "] Running kproto clustering for k = ", k, ", lambda = ", lambda_name, ", distance = ", distance, "\n")
      
      #2.322 Start timing
      start_time <- Sys.time()
      
      #2.323 Run kproto clustering
      if (is.null(lambda_values)) {
        
        #2.3231 Default lambda clustering 
        kproto_model <- clustMixType::kproto(
          x = risk_variable_clustering_data,
          k = k,
          type = distance,
          kp_obj = "all",
          nstart = 8,
          verbose = FALSE
        )
      } else {
        
        #2.3232 Adjusted lambda clustering
        kproto_model <- clustMixType::kproto(
          x = risk_variable_clustering_data,
          k = k,
          lambda = lambda_values,
          type = distance,
          kp_obj = "all",
          nstart = 8,
          verbose = FALSE
        )
      }
      
      #2.324 Calculate silhouette scores using cluster::silhouette and cluster::daisy
      silhouette_score <- cluster::silhouette(
        x = kproto_model$cluster,
        dist = cluster::daisy(risk_variable_clustering_data, metric = "gower")
      )
      
      #2.325 Calculate average silhouette values
      average_silhouette <- mean(silhouette_score[, "sil_width"])
      
      #2.326 Log silhouette scores
      silhouette_scores <- rbind(
        silhouette_scores,
        data.frame(
          k = k,
          lambda_type = lambda_name,
          distance = distance,
          silhouette_score = average_silhouette
        )
      )
      
      #2.327 End timing
      end_time <- Sys.time()
      runtime_seconds <- as.numeric(difftime(end_time, start_time, units = "secs"))
      
      #2.328 Log runtime
      runtime_log <- runtime_log %>%
        add_row(method = "silhouette", distance = distance, lambda_type = lambda_name, RuntimeSeconds = runtime_seconds)
      
      #2.329 Store clustering results
      result_name <- paste0("k_", k, "_", lambda_name, "_", distance)
      k_optimization_indices[[result_name]] <- list(
        model = kproto_model,
        silhouette = average_silhouette,
        runtime = runtime_seconds
      )
    }
  }
}

#3. Output
#3.1 Save the runtime log
write.csv(runtime_log, "../../results/main_analysis/k_optimization_runtime_log.csv", row.names = FALSE)
cat("Runtime log saved successfully\n")

#3.2 Save the optimization indices
saveRDS(k_optimization_indices, file = "../../results/main_analysis/k_optimization_indices.rds")
cat("Optimization indices saved successfully\n")

```

### Using Cluster Validation Indices to Select a k Value, Distance Method, and lambda Parameters

```{r parameter selection, echo = FALSE, warning = FALSE}

## Determine the Optimal Number of Clusters (k), Distance Method, and Lambda Parameters to Group Risk Variables ##

#1. Extract relevant results for evaluation of optimal k + distance + 
#1.1 Initialize placeholders for summary results
optimal_results <- data.frame(
  lambda_type = character(),
  distance = character(),
  k = integer(),
  silhouette_score = numeric(),
  stringsAsFactors = FALSE
)

#1.2 Iterate through silhouette_scores to find the optimal combinations
for (lambda_name in unique(silhouette_scores$lambda_type)) {
  for (distance in unique(silhouette_scores$distance)) {
    
    #1.21 Filter results for the current lambda and distance
    subset_scores <- silhouette_scores %>%
      filter(lambda_type == lambda_name & distance == distance & k != 2 & k != 3)
    
    #1.22 Find the optimal k (highest silhouette score)
    best_score <- max(subset_scores$silhouette_score, na.rm = TRUE)
    best_k <- subset_scores$k[which.max(subset_scores$silhouette_score)]
    
    #1.23 Store the result
    optimal_results <- rbind(
      optimal_results,
      data.frame(
        lambda_type = lambda_name,
        distance = distance,
        k = best_k,
        silhouette_score = best_score
      )
    )
  }
}

#2. Rank the top solutions
#2.1 Sort the optimal_results dataframe by silhouette score in descending order
ranked_results <- optimal_results %>%
  arrange(desc(silhouette_score))

#2.2 Extract the top 5 solutions
top_solutions <- ranked_results[1:5, ]

#2.3 Assign ranks to the solutions for easier interpretation
top_solutions <- top_solutions %>%
  mutate(rank = row_number())

#2.4 Print the top solutions for detailed inspection
kable(top_solutions, digits = 2, caption = "5 Highest k + distance + lambda Silhouette Combinations")

#3. Create plots visualizing the results to aid in determining optimal k
#3.1 Generate the elbow plot
#3.11 Prepare data for the elbow plot
cluster_validation_wss <- data.frame(
  k = integer(),
  Tot_WithinSS = numeric(),
  lambda_type = character(),
  distance = character(),
  stringsAsFactors = FALSE
)

#3.12 Iterate through all combinations of k, lambda, and distance
for (lambda_name in unique(silhouette_scores$lambda_type)) {
  for (distance in unique(silhouette_scores$distance)) {
    for (k in k_range) {
      
      #3.121 Extract the WSS value for the current combination
      result_key <- paste0("k_", k, "_", lambda_name, "_", distance)
      if (!is.null(k_optimization_indices[[result_key]])) {
        wss_value <- k_optimization_indices[[result_key]][["model"]][["tot.withinss"]]
        cluster_validation_wss <- rbind(
          cluster_validation_wss,
          data.frame(
            k = k,
            Tot_WithinSS = wss_value,
            lambda_type = lambda_name,
            distance = distance
          )
        )
      }
    }
  }
}

#3.13 Create the facet elbow plot
faceted_elbow_plot <- ggplot(cluster_validation_wss, aes(x = k, y = Tot_WithinSS)) +
  geom_line(aes(color = paste(lambda_type, distance, sep = " | ")), size = 1) +
  geom_point(aes(color = paste(lambda_type, distance, sep = " | ")), size = 2) +
  facet_wrap(~ lambda_type + distance, scales = "free_y", ncol = 2) +
  labs(
    title = "Elbow Plot for Within-Cluster Sum of Squares (WSS)",
    x = "Number of Clusters (k)",
    y = "Total Within-Cluster Sum of Squares",
    color = "Lambda | Distance"
  ) +
  theme_minimal(base_size = 12) +
  theme(strip.text = element_text(size = 10, face = "bold"))

#3.2 Facet silhouette plot
faceted_silhouette_plot <- ggplot(silhouette_scores, aes(x = k, y = silhouette_score)) +
  geom_line(aes(color = paste(lambda_type, distance, sep = " | ")), size = 1) +
  geom_point(aes(color = paste(lambda_type, distance, sep = " | ")), size = 2) +
  facet_wrap(~ lambda_type + distance, scales = "free_y", ncol = 2) +
  labs(
    title = "Faceted Silhouette Plot for Lambda and Distance Combinations",
    x = "Number of Clusters (k)",
    y = "Silhouette Score",
    color = "Lambda | Distance"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    strip.text = element_text(size = 10, face = "bold"),
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    axis.title = element_text(size = 12)
  )

```

**1. Optimal Clustering Parameters**

The updated code evaluates silhouette scores and within-cluster sum of squares (WSS) for each combination of:

- *Lambda configurations*: Averaged, normalized, and grid-searched values conpared to default lambda parameters

- *Distance metrics*: Huang and Gower.

- *Number of clusters (`k`)*: Evaluated across `k = 4–12` (excluding `k = 2` and `k = 3` to avoid biased high scores for smaller cluster solutions).

2. **Optimal Solutions**: The top five configurations are ranked by silhouette scores to provide alternative options beyond the single best solution.

3. **Performance Summary**:

   - The best configuration for clustering is selected based on the highest silhouette score among all combinations.
   
   - Rankings include distance methods and lambda values for detailed insights into parameter effectiveness.

```{r optimal results, echo = FALSE, warning = FALSE}

# Display the top-ranked solutions in line
kable(top_solutions, digits = 2, caption = "Top 5 Ranked Clustering Solutions Based on Silhouette Scores")

```

**4. Comparison of Distance Metrics and Lambda Configurations**

Overview:

- Distance metrics and lambda configurations are compared to identify trends in silhouette scores and WSS values.

- Comparisons are visualized to evaluate clustering stability and interpretability.

Visualization Enhancements:

- *Facet Elbow Plot*: Displays the WSS values for each combination of lambda and distance metrics.

- *Facet Silhouette Plot*: Illustrates silhouette scores across `k` for all combinations, highlighting trends for each parameter setting.

```{r cluster validation plots, echo = FALSE, warning = FALSE}

# Print the facet elbow plot
print(faceted_elbow_plot)

# Print the facet silhouette plot
print(faceted_silhouette_plot)

```
