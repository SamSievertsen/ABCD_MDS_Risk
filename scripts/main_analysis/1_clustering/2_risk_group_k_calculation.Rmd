---
title: "Bipolar Disorder & Suicidality Risk Group Clustering k Calculation"
author: "Sam A. Sievertsen"
date: "`r Sys.Date()`"
output:
  html_document:
    self_contained: true
params:
  scaling_method: "robust"  # editable options: none, z_score, min_max, percentile, max_absolute, robust
---

```{r global, include = FALSE}

# Set global env variables
knitr::opts_chunk$set(echo = TRUE, cache = FALSE, message = TRUE, warning = TRUE, results = "markup", verbose = TRUE, comment = "")

```

```{r environment, echo = FALSE, include = FALSE, warning = FALSE}

# Load necessary packages + environment
library(knitr)
library(dplyr)
library(tidyr)
library(DescTools)
library(skimr)
library(easystats)
library(cluster)
library(fpc)
library(clustMixType)
library(factoextra)
library(FactoMineR)
library(NbClust)
library(clValid) 
library(ClusterR) 
library(mclust)
library(clusterSim)
library(clustertend)
library(hopkins)
library(tidymodels)
library(tidyclust)
library(tidyquant)
library(FeatureImpCluster)
library(ggplot2)
library(inflection)
library(furrr)
library(purrr)
library(glue)
library(progressr)
handlers("txtprogressbar")

# Parallel & numeric options
plan(multicore, workers = parallel::detectCores())
options(future.globals.maxSize = 5e9)
options(scipen = 999, digits = 8)

## Logging / checkpoint locations ##

# Grab SLURM job name (or default when local)
SLURM_JOB_NAME <- Sys.getenv("SLURM_JOB_NAME", "local_kcalc")

# Create detailed per‐step log tailed by sbatch wrapper
log_file <- Sys.getenv(
  "DETAILED_LOG",
  file.path(getwd(), paste0(SLURM_JOB_NAME, "_detail.log"))
)

# Create high-level summary CSV
summary_csv <- Sys.getenv(
  "SUMMARY_CSV",
  file.path(getwd(), paste0(SLURM_JOB_NAME, "_summary.csv"))
)

# Make sure parent log folders exist & create if not
dir.create(dirname(log_file), recursive = TRUE, showWarnings = FALSE)
dir.create(dirname(summary_csv), recursive = TRUE, showWarnings = FALSE)

# Initialize summary CSV header if missing
if (!file.exists(summary_csv)) {
  cat("method,status,start_time,end_time,duration_min\n",
      file = summary_csv)
}

## Reproducibility & parameters ##

# Set seed for future reproducible random starts
set.seed(123)

# Set scaling method param as object in env
scaling_method <- params$scaling_method  

## Establish project paths & read in data ## 

# List project repository location
REPO <- "/home/exacloud/gscratch/NagelLab/staff/sam/projects/ABCD_MDS_Risk"

# List path to checkpoint dirs for kproto + validation RDS
kproto_dir <- file.path(REPO, "data/data_processed", "kproto_results")
validation_dir <- file.path(REPO, "data/data_processed", "validation_results")

# Create checkpoint dirs for kproto + validation RDS if they don't already exist
if (!dir.exists(kproto_dir)) {dir.create(kproto_dir, recursive = TRUE, showWarnings = FALSE)}
if (!dir.exists(validation_dir)) {dir.create(validation_dir, recursive = TRUE, showWarnings = FALSE)}

# Read in risk variable data to be used in clustering
risk_variable_data <- read.csv(
  file.path(REPO, "data/data_processed", "risk_variable_data.csv")
)

```

## Data Prep for k Value Analysis 

Include summary of what we are doing, which scaling method was chosen and why, etc.

```{r data prep, warning = FALSE}

## Data Prep ## 

#1. Ensure dichotimization of categorical data for clustering
risk_variable_data <- risk_variable_data %>% 
  mutate(across(c("family_history_depression", "family_history_mania", "bullying"), as.factor))

#2. Retain subject IDs as row names while removing variables that will not be clustered 
#2.1 Remove all variables except subject ID from the dataframe for clustering
risk_variable_clustering_data <- risk_variable_data %>%
  dplyr::select(-session_id, -family_id, -site, -sex, -age, -race_ethnicity)

#2.2 Set subject IDs as row names
row.names(risk_variable_clustering_data) <- risk_variable_clustering_data$participant_id

#2.3 Remove subject ID column
risk_variable_clustering_data <- risk_variable_clustering_data %>% 
  dplyr::select(-participant_id)

#3. Identify continuous and categorical variables
continuous_vars <- names(risk_variable_clustering_data)[sapply(risk_variable_clustering_data, is.numeric)]
categorical_vars <- names(risk_variable_clustering_data)[sapply(risk_variable_clustering_data, is.factor)]

#4. Scaling function to be matched with established param
apply_scaling <- function(df, method) {
  out <- df
  if (method != "none") {
    cont <- df[continuous_vars]
    scaled <- switch(method,
      z_score = scale(cont),
      min_max = purrr::map_df(cont, ~ (.x - min(.x, na.rm=TRUE)) / (max(.x, na.rm=TRUE) - min(.x, na.rm=TRUE))),
      percentile = purrr::map_df(cont, ~ rank(.x, na.last="keep") / sum(!is.na(.x)) * 100),
      max_absolute = purrr::map_df(cont, ~ .x / max(abs(.x), na.rm=TRUE)),
      robust = purrr::map_df(cont, ~ (.x - median(.x, na.rm=TRUE)) / (IQR(.x, na.rm=TRUE) %||% 1))
    )
    out[continuous_vars] <- scaled
  }
  out
}

#5. Scale data according to param set in script call
scaled_data <- apply_scaling(risk_variable_clustering_data, params$scaling_method)

```

## Determining Optimal Number of Clusters (*k* Value)

Choosing the appropriate number of clusters (*k*) is critical for uncovering meaningful groupings in mixed‐type data. Here, we combine descriptive elbow analysis with multiple internal validation indices to arrive at a robust, transparent recommendation for *k* between 2 and 8:

1. **Elbow method**
   We plot the within‐cluster sum of squares (WCSS) across *k* = 2:8 and apply the Kneedle algorithm \[Satopaa *et al.*, 2011] via the `inflection` package (Emekes, 2017) to quantitatively identify the “elbow” point. This provides a visual and numeric guide, but is treated as descriptive rather than prescriptive for purposes of our analyses

2. **Internal validation indices**
   Using `validation_kproto()`, we compute six statistical validation indices: Silhouette, C-Index, Dunn, Gamma, Point-biserial, and Tau—for each *k*. Each index captures a different aspect of cluster compactness and separation without requiring true labels

3. **Consensus evaluation**
   We record the optimal *k* returned by each index and examine patterns of agreement or discrepancy. A convergence of multiple indices on the same *k* will strengthen our confidence in that choice; though a nuanced examination of index scores, the distribution of index scores, and the elbow method will be consulted to 

Below, we implement this three‐step procedure. First, we generate the elbow plot and run Kneedle to flag an elbow. Next, we loop through each validation index to extract its preferred *k*. Finally, we summarize all results in a single table for comparison

```{r optimal k, warning = FALSE}

## Determining Optimal Number of Clusters (*k* Value) ##

#1. Generate clustering solutions & validation indices of interest
#1.1 Establish relevant parameters & paths
#1.1.1 Create a vector containing the k’s to test
k_vals <- 2:8

#1.1.2 List where to stash / load the kproto list for this method
proto_file <- file.path(kproto_dir, sprintf("kproto_%s.rds", scaling_method))

#1.1.3 List where to stash / load full validation_kproto() results
indices <- c("silhouette","cindex","gamma","ptbiserial","tau")
val_files <- file.path(validation_dir, sprintf("val_%s_%s.rds", scaling_method, indices))

#1.2 Load or compute all k-prototypes clusters
if (file.exists(proto_file)) {
  
  #1.2.1 Skip clustering if already done
  kp_list <- readRDS(proto_file)
  cat(sprintf("%s|PROTO_SKIP|%s|loaded %d ks from cache\n",
              format(Sys.time(), "%Y-%m-%d %H:%M:%OS3"),
              scaling_method, length(k_vals)),
      file = log_file, append = TRUE)
} else {
  
  #1.2.2 Compute clusters in parallel across k if not completed
  proto_start <- Sys.time()
  cat(sprintf("%s|PROTO_START|%s\n",
              format(proto_start, "%Y-%m-%d %H:%M:%OS3"), scaling_method),
      file = log_file, append = TRUE)
  
  #1.2.3 Store a list containing each kp object (parallel over 7 k’s)
  kp_list <- future_map(
    k_vals,
    ~ kproto(x = scaled_data,
             k = .x,
             nstart = 8,
             verbose = TRUE),
    .options = furrr_options(seed = TRUE)
  )
  names(kp_list) <- paste0("k", k_vals)
  saveRDS(kp_list, proto_file)
  
  #1.2.4 Log status for each k  
  for (j in seq_along(k_vals)) {  
    cat(sprintf("%s|WSS|%s|k=%d|wss=%.1f\n",
                format(Sys.time(), "%Y-%m-%d %H:%M:%OS3"),
                scaling_method, k_vals[j]),
        file = log_file, append = TRUE)  
  }
  
  proto_end <- Sys.time()
  cat(sprintf("%s|PROTO_END|%s|duration=%.1fmin\n",
              format(proto_end, "%Y-%m-%d %H:%M:%OS3"),
              scaling_method,
              as.numeric(difftime(proto_end, proto_start, units = "mins"))),
      file = log_file, append = TRUE)
}

#1.2.6 Extract total within-cluster sum of squares (WSS) for elbow plot  
wss_vec <- map_dbl(kp_list, "tot.withinss")
names(wss_vec) <- k_vals

#1.3 Run each validation index in parallel
res_list <- future_map_dfr(
  indices,
  function(idx) {
    val_f <- sprintf("val_%s_%s.rds", scaling_method, idx)
    val_p <- file.path(validation_dir, val_f)
    
    if (file.exists(val_p)) {
      
      #1.3.1 Reload cached validation if it already exists
      vr_all <- readRDS(val_p)
      cat(sprintf("%s|VAL_SKIP|%s|%s|k_opt=%d\n",
                  format(Sys.time(), "%Y-%m-%d %H:%M:%OS3"),
                  scaling_method, idx, vr_all$k_opt),
          file = log_file, append = TRUE)
    } else {
      
      #1.3.2 Compute full validation for each index if it doesn't already exist
      idx_start <- Sys.time()
      cat(sprintf("%s|VAL_START|%s|%s\n",
                  format(idx_start, "%Y-%m-%d %H:%M:%OS3"),
                  scaling_method, idx),
          file = log_file, append = TRUE)
      
      vr_all <- validation_kproto(
        method = idx,
        data = scaled_data,
        k = k_vals,
        kp_obj = "all",
        nstart = 8,
        verbose = FALSE
      )
      saveRDS(vr_all, val_p)
      
      idx_end <- Sys.time()
      cat(sprintf("%s|VAL_END|%s|%s|k_opt=%d|duration=%.1fsec\n",
                  format(idx_end, "%Y-%m-%d %H:%M:%OS3"),
                  scaling_method, idx, vr_all$k_opt,
                  as.numeric(difftime(idx_end, idx_start, units = "secs"))),
          file = log_file, append = TRUE)
    }
    
    #1.3.3 Return results for each index
    tibble(
      method = scaling_method,
      index = idx,
      k_opt = vr_all$k_opt,
      index_val = vr_all$indices[which(k_vals == vr_all$k_opt)]
    )
  },
  .options = furrr_options(seed = TRUE)
)

#1.4 Stash a copy of the summarized index results to later display 
summary_tbl <- bind_rows(res_list)

#2. Generate elbow diagnostics & Kneedle (UIK)  
#2.1 Prepare a data.frame for plotting
elbow_df <- tibble(
  k = as.integer(names(wss_vec)),
  wss = as.numeric(wss_vec)
)

#2.2 Create first basic elbow plot
#2.2.1 Generate basic plot
elbow_plot <- ggplot(elbow_df, aes(x = k, y = wss)) +
  geom_line(size = 1) +
  geom_point(size = 3) +
  labs(
    x = "Number of clusters (k)",
    y = "Total within‐cluster sum of squares",
    title = "Elbow plot: WSS vs k") +
  theme_minimal(base_size = 14)

#2.2.2 Print the basic elbow plot
print(elbow_plot)

#2.3 Find the “knee” via the Kneedle (UIK) algorithm
knee <- uik(elbow_df$k, elbow_df$wss)

#2.3.1 Log the UIK elbow into the detailed log
cat(
  sprintf(
    "%s|UIK_ELBOW|%s|k=%d\n", format(Sys.time(), "%Y-%m-%d %H:%M:%OS3"),
    scaling_method, knee),
  file = log_file, append = TRUE)

#2.4 Create a more detailed elbow plot with UIK annotation
#2.4.1 Generate the elbow plot with the UIK
elbow_plot_uik <- ggplot(elbow_df, aes(x = k, y = wss)) +
  geom_line(size = 1) +
  geom_point(size = 3) +
  geom_vline(
    xintercept = knee,
    linetype = "dashed",
    size = 0.8,
    color = "red") +
  annotate(
    "text",
    x = knee,
    y = max(elbow_df$wss) * 0.95,
    label = paste0("UIK elbow:\nk = ", knee),
    hjust = 0,
    size = 5,
    color = "red") +
  labs(
    x = "Number of clusters (k)",
    y = "Total WSS",
    title = "Elbow plot with Kneedle (UIK) selection") +
  theme_minimal(base_size = 14)

#2.4.2 Print the elbow plot with the UIK
print(elbow_plot_uik)

#3. Summarize & visualize validation indices & pick a consensus k 
#3.1 Define which direction is “better” for each index
dir_tbl <- tribble(
  ~index, ~direction, ~label,
  "silhouette", "higher", "Silhouette (higher = better)",
  "cindex", "lower", "C-index (lower = better)",
  "gamma", "higher", "Gamma (higher = better)",
  "ptbiserial", "higher", "Point-biserial (higher = better)",
  "tau", "higher", "Tau (higher = better)"
)

#3.2 Pull out full index‐vs‐k curves from cached validation objects
index_k_df <- purrr::map_dfr(
  indices,
  function(idx) {
    vr_all <- readRDS(file.path(validation_dir, sprintf("val_%s_%s.rds", scaling_method, idx)))
    ks <- as.integer(names(vr_all$indices))
    vals <- as.numeric(vr_all$indices)
    tibble(index = idx, k = ks, value = vals)
  }
  ) %>%
  left_join(dir_tbl, by = "index")

#3.3 Compute each index’s best k
summary_tbl <- index_k_df %>%
  group_by(index, direction, label) %>%
  summarise(
    k_opt = if (direction[1] == "lower") k[which.min(value)] else k[which.max(value)],
    index_opt = if (direction[1] == "lower") min(value)       else max(value),
    .groups = "drop"
  )

#3.3.1 Determine “consensus” k (at least according to simple vote)
consensus_k <- summary_tbl$k_opt %>% table() %>% which.max() %>% names() %>% as.integer()

#3.3.2 Print and log the "consensus k"
cat(sprintf("%s|CONSENSUS_K|%s|%d\n", format(Sys.time(), "%Y-%m-%d %H:%M:%OS3"),
            scaling_method, consensus_k),
    file = log_file, append = TRUE)

#3.4 Create a line plot of all indices vs k
#3.4.1 Generate the line plot for each index x k value
index_line_plot <- ggplot(index_k_df, aes(x = k, y = value, color = label)) +
  geom_line(size = 1) +
  geom_point(size = 2) +
  geom_vline(xintercept = consensus_k, linetype = "dashed", color = "grey50") +
  annotate(
    "text", x = consensus_k + 0.2,
    y = max(index_k_df$value)*0.95,
    label = paste0("Consensus k = ", consensus_k),
    hjust = 0, size = 5) +
  labs(
    x = "Number of clusters (k)",
    y = "Validation-index value",
    color = "Index & direction",
    title = glue::glue("Validation-index trajectories ({scaling_method} scaling)")) +
  theme_minimal(base_size = 14)

#3.4.2 Print the line plot for each index x k value
print(index_line_plot)

#3.5 Create a heatmap of index values x each k
#3.5.1 Generate the heatmap
index_heatmap <- ggplot(index_k_df, aes(x = factor(k), y = label, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_viridis_c(option = "magma") +
  labs(
    x = "k",
    y = NULL,
    fill = "Value",
    title = "Heatmap of validation-index values") +
  theme_minimal(base_size = 14) +
  theme(axis.text.x = element_text(vjust = 0.5))

#3.5.2 Print the heatmap
print(index_heatmap)

#3.6 List the tabular summary of each index's optimal k & value
summary_tbl %>%
  arrange(label) %>%
  dplyr::select(label, k_opt, index_opt) %>%
  knitr::kable(
    caption = glue::glue("Optimal k per index ({scaling_method} scaling)"),
    col.names = c("Index (direction)", "optimal k", "Value"),
    digits = 3) %>%
  kableExtra::kable_styling(bootstrap_options = c("striped","hover","condensed","responsive"))

```

**Interpretation:**

- The elbow plot suggests a bend at *k* = `r print(as.numeric(knee))`, reinforced by Kneedle’s quantitative estimate

- Among the `r nrow(summary_tbl)` validation indices, `r sum(summary_tbl$k_opt == consensus_k)` (`r paste(summary_tbl$label[summary_tbl$k_opt == consensus_k], collapse = ", ")`) also vote for *k* = `r consensus_k`, while the others (`r paste0(summary_tbl$label[summary_tbl$k_opt != consensus_k], " (k=", summary_tbl$k_opt[summary_tbl$k_opt != consensus_k], ")", collapse = ", ")`) peak at neighboring values 

- Taken together, these results indicate that *k* = `r consensus_k` balances cohesion and separation most consistently in our risk variable data
