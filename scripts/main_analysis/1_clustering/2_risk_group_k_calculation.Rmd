---
title: "Bipolar Disorder & Suicidality Risk Group Clustering k Calculation"
author: "Sam A. Sievertsen"
date: "`r Sys.Date()`"
output:
  html_document:
    self_contained: true
params:
  validation_index: "all"
  scaling_method: "robust"
---

```{r global, include = FALSE}

# Set global env variables
knitr::opts_chunk$set(echo = TRUE, cache = FALSE, message = TRUE, warning = TRUE, results = "markup", verbose = TRUE, comment = "")

```

```{r environment, echo = FALSE, include = FALSE, warning = FALSE}

# Load necessary packages + environment
library(knitr)
library(dplyr)
library(tidyr)
library(DescTools)
library(skimr)
library(easystats)
library(cluster)
library(fpc)
library(clustMixType)
library(factoextra)
library(FactoMineR)
library(NbClust)
library(clValid) 
library(ClusterR) 
library(mclust)
library(clusterSim)
library(clustertend)
library(hopkins)
library(tidymodels)
library(tidyclust)
library(tidyquant)
library(FeatureImpCluster)
library(ggplot2)
library(inflection)
library(furrr)
library(purrr)
library(glue)
library(progressr)
handlers("txtprogressbar")

# Set up logging from SLURM script
log_file <- Sys.getenv("DETAILED_LOG")
summary_csv <- Sys.getenv("SUMMARY_CSV")

# Denote which validation index this array‐task will run
idx <- as.character(params$validation_index)

# Disable future multicore/furrr here as SLURM array currently provides parallelism
plan(sequential)

# Set numerical options
options(scipen = 999, digits = 8)

# Grab SLURM job name (or default when local)
SLURM_JOB_NAME <- Sys.getenv("SLURM_JOB_NAME", "local_kcalc")

# Make sure parent log folders exist & create if not then create them
dir.create(dirname(log_file), recursive = TRUE, showWarnings = FALSE)
dir.create(dirname(summary_csv), recursive = TRUE, showWarnings = FALSE)

# Initialize summary csv header if missing
if (!file.exists(summary_csv)) {
  cat("method,status,start_time,end_time,duration_min\n",
      file = summary_csv)
}

# Set seed for future reproducible random starts
set.seed(123)

# Set scaling method from piped params argument
scaling_method <- params$scaling_method

# List project repository location
REPO <- "/home/exacloud/gscratch/NagelLab/staff/sam/projects/ABCD_MDS_Risk"

# List path to checkpoint dirs for kproto + validation RDS
kproto_dir <- file.path(REPO, "data/data_processed", "kproto_results")
validation_dir <- file.path(REPO, "data/data_processed", "validation_results")

# Create checkpoint dirs for kproto + validation RDS if they don't already exist
if (!dir.exists(kproto_dir)) {dir.create(kproto_dir, recursive = TRUE, showWarnings = FALSE)}
if (!dir.exists(validation_dir)) {dir.create(validation_dir, recursive = TRUE, showWarnings = FALSE)}

# Read in risk variable data for clustering
risk_variable_data <- read.csv(
  file.path(REPO, "data/data_processed", "risk_variable_data.csv")
)

```

## Data Prep for k Value Analysis 

Include summary of what we are doing, which scaling method was chosen and why, etc.

```{r data prep, warning = FALSE}

## Data Prep ## 

#1. Ensure dichotimization of categorical data for clustering
risk_variable_data <- risk_variable_data %>% 
  mutate(across(c("family_history_depression", "family_history_mania", "bullying"), as.factor))

#2. Retain subject IDs as row names while removing variables that will not be clustered 
#2.1 Remove all variables except subject ID from the dataframe for clustering
risk_variable_clustering_data <- risk_variable_data %>%
  dplyr::select(-session_id, -family_id, -site, -sex, -age, -race_ethnicity)

#2.2 Set subject IDs as row names
row.names(risk_variable_clustering_data) <- risk_variable_clustering_data$participant_id

#2.3 Remove subject ID column
risk_variable_clustering_data <- risk_variable_clustering_data %>% 
  dplyr::select(-participant_id)

#3. Identify continuous and categorical variables
continuous_vars <- names(risk_variable_clustering_data)[sapply(risk_variable_clustering_data, is.numeric)]
categorical_vars <- names(risk_variable_clustering_data)[sapply(risk_variable_clustering_data, is.factor)]

#4. Scaling function to be matched with established param
apply_scaling <- function(df, method) {
  out <- df
  if (method != "none") {
    cont <- df[continuous_vars]
    scaled <- switch(method,
      z_score = scale(cont),
      min_max = purrr::map_df(cont, ~ (.x - min(.x, na.rm=TRUE)) / (max(.x, na.rm=TRUE) - min(.x, na.rm=TRUE))),
      percentile = purrr::map_df(cont, ~ rank(.x, na.last="keep") / sum(!is.na(.x)) * 100),
      max_absolute = purrr::map_df(cont, ~ .x / max(abs(.x), na.rm=TRUE)),
      robust = purrr::map_df(cont, ~ (.x - median(.x, na.rm=TRUE)) / (IQR(.x, na.rm=TRUE) %||% 1))
    )
    out[continuous_vars] <- scaled
  }
  out
}

#5. Scale data according to param set in script call
scaled_data <- apply_scaling(risk_variable_clustering_data, params$scaling_method)

```

## Overview of Determining Optimal Number of Clusters (*k* Value)

Choosing the appropriate number of clusters (*k*) is critical for uncovering meaningful groupings in mixed‐type data. In this analysis, we combine descriptive elbow analysis with multiple internal validation indices to arrive at a robust, transparent recommendation for *k* between 2 and 8:

1. **Elbow method**
   We plot the within‐cluster sum of squares (WCSS) across *k* = 2:8 and apply the Kneedle algorithm \[Satopaa *et al.*, 2011] via the `inflection` package (Emekes, 2017) to quantitatively identify the “elbow” point. This provides a visual and numeric guide, but is treated as descriptive rather than prescriptive for purposes of our analyses

2. **Internal validation indices**
   Using `validation_kproto()`, we compute six statistical validation indices: Silhouette, C-Index, Dunn, Gamma, Point-biserial, and Tau—for each *k*. Each index captures a different aspect of cluster compactness and separation without requiring true labels

3. **Consensus evaluation**
   We record the optimal *k* returned by each index and examine patterns of agreement or discrepancy. A convergence of multiple indices on the same *k* will strengthen our confidence in that choice; though a nuanced examination of index scores, the distribution of index scores, and the elbow method will be consulted to 

## Implementation: parallel k-prototypes + single‐index validation

In this step we:

  1. Compute k-prototypes clusterings for *k* = 2:8 once and cache the results  
  
  2. Extract within-cluster sum of squares (WSS) for elbow diagnostics  
  
  3. Run exactly one internal validation index per SLURM task (silhouette, C-index, gamma, point-biserial or Tau)  
  
  4. Log start/end times, WSS values, and per-index *k*ₒₚₜ for reproducibility  

This modular, SLURM-array approach maximizes parallelism (one R process per index) while ensuring all results are captured in detailed logs

```{r validation, include = FALSE, warning = FALSE, message = FALSE, eval = (idx != "all")}

## Run kproto and validation for a single index and then exit ##

#1.1 k values & file paths
k_vals <- 2:8
proto_file <- file.path(kproto_dir, sprintf("kproto_%s.rds", scaling_method))
validation_file <- file.path(validation_dir, sprintf("val_%s_%s.rds", scaling_method, idx))

#1.2 Load or compute k‐prototype list
if (! file.exists(proto_file)) {
  
  #1.2.1 Start the kproto timer
  proto_start <- Sys.time()
  cat(
    sprintf("%s|PROTO_START|%s\n", format(proto_start, "%Y-%m-%d %H:%M:%OS3"),
            scaling_method),
    file = log_file, append = TRUE
  )
  
  #1.2.2 Execute kproto & create the list if doesn't already exist
  kp_list <- future_map(
    k_vals,
    ~ kproto(x = scaled_data,
             k = .x,
             nstart = 8,
             verbose = TRUE),
    .options = furrr_options(seed = TRUE)
  )
  names(kp_list) <- paste0("k", k_vals)
  saveRDS(kp_list, proto_file)
  
  #1.2.3 End the kproto timer
  proto_end <- Sys.time()
  cat(
    sprintf("%s|PROTO_END|%s|duration=%.1fmin\n", format(proto_end, "%Y-%m-%d %H:%M:%OS3"),
            scaling_method, as.numeric(difftime(proto_end, proto_start, units = "mins"))),
    file = log_file, append = TRUE
  )

  #1.2.4 Read in the kproto list if already computed
} else {
  kp_list <- readRDS(proto_file)
  cat(
    sprintf("%s|PROTO_SKIP|%s|loaded %d ks from cache\n",
            format(Sys.time(), "%Y-%m-%d %H:%M:%OS3"),
            scaling_method,
            length(k_vals)),
    file = log_file, append = TRUE
  )
}

#1.3 Extract WSS
wss_vec <- map_dbl(kp_list, "tot.withinss")
names(wss_vec) <- k_vals
for (j in seq_along(k_vals)) {
  cat(
    sprintf("%s|WSS|%s|k=%d|wss=%.1f\n",
            format(Sys.time(), "%Y-%m-%d %H:%M:%OS3"),
            scaling_method,
            k_vals[j],
            wss_vec[j]),
    file = log_file, append = TRUE
  )
}

#1.4 Run the one‐index validation
idx_start <- Sys.time()
cat(sprintf("%s|VAL_START|%s|%s\n", format(idx_start, "%Y-%m-%d %H:%M:%OS3"),
          scaling_method, idx),
  file = log_file, append = TRUE
)

#1.4.1 If validation already exists, read in and skip computing - start validation timer regardless
if (file.exists(validation_file)) {
  vr_all <- readRDS(validation_file)
  cat(sprintf("%s|VAL_SKIP|%s|%s|k_opt=%d\n", format(Sys.time(), "%Y-%m-%d %H:%M:%OS3"),
            scaling_method, idx, vr_all$k_opt),
    file = log_file, append = TRUE
  )

  #1.4.2 If validation index doesn't exist, compute independently in this job 
} else {
  vr_all <- validation_kproto(
    method = idx,
    object = kp_list,
    kp_obj = "all",
    verbose = FALSE
  )
  
  #1.4.3 Save the validation index results
  saveRDS(vr_all, validation_file)

  #1.4.4 End the index timer and store in log
  idx_end <- Sys.time()
  cat(
    sprintf("%s|VAL_END|%s|%s|k_opt=%d|duration=%.1fsec\n", format(idx_end, "%Y-%m-%d %H:%M:%OS3"),
            scaling_method,
            idx,
            vr_all$k_opt,
            as.numeric(difftime(idx_end, idx_start, units = "secs"))),
    file = log_file, append = TRUE
  )
}

#1.5 Write out the per‐index summary to the CSV
cat(
  sprintf("%s,validation_%s,%s,%s,%.1f\n", scaling_method, idx,
          format(idx_start, "%Y-%m-%d %H:%M:%OS3"), format(Sys.time(), "%Y-%m-%d %H:%M:%OS3"),
          as.numeric(difftime(Sys.time(), idx_start, units = "mins"))),
  file = summary_csv, append = TRUE
)

#1.6 now bail out: we do *not* want to run elbow or plots in this job
quit(save = "no", status = 0)

```

## Checkpoint: aggregate diagnostics & prepare for consensus

All five index‐specific jobs have now finished and written:

- A single cached k-prototype list (k = 2:8)  

- WSS values for each *k*  

- Per-index validation objects (`val_<method>_<index>.rds`) with optimal *k*

Next, we will:

  1. Read and combine all WSS into an elbow plot and run Kneedle  

  2. Merge each index’s full *k*-vs-score trajectories  

  3. Compute each index’s preferred *k* and select a consensus *k*  

  4. Visualize results via line plots and heatmaps for final inspection

```{r optimal k, warning = FALSE, eval = (idx == "all")}

## Determining Optimal Number of Clusters (*k* Value) ##

#1. Generate clustering solutions & validation indices of interest
#1.1 Establish relevant parameters & paths
#1.1.1 Create a vector containing the k’s to test
k_vals <- 2:8

#1.1.2 List where to stash / load the kproto list for this method
proto_file <- file.path(kproto_dir, sprintf("kproto_%s.rds", scaling_method))

#1.2 Load or compute all k-prototypes clusters (unchanged)
if (file.exists(proto_file)) {
  
  #1.2.1 Skip clustering if already done
  kp_list <- readRDS(proto_file)
  cat(
    sprintf("%s|PROTO_SKIP|%s|loaded %d ks from cache\n", format(Sys.time(), "%Y-%m-%d %H:%M:%OS3"),
            scaling_method, length(k_vals)),
    file = log_file, append = TRUE
  )
  
} else {
  
  #1.2.2 Compute clusters in parallel across k if not completed
  proto_start <- Sys.time()
  cat(
    sprintf("%s|PROTO_START|%s\n",
            format(proto_start, "%Y-%m-%d %H:%M:%OS3"),
            scaling_method),
    file = log_file, append = TRUE
  )
  
  #1.2.3 Store a list containing each kp object (parallel over 7 k’s)
  kp_list <- future_map(
    k_vals,
    ~ kproto(x = scaled_data,
             k = .x,
             nstart  = 8,
             verbose = TRUE),
    .options = furrr_options(seed = TRUE)
  )
  
  #1.2.4 Provide a k suffix to each of the k values
  names(kp_list) <- paste0("k", k_vals)
  
  #1.2.5 Save the kproto list
  saveRDS(kp_list, proto_file)
  
  #1.2.6 Log end status for clustering
  proto_end <- Sys.time()
  cat(
    sprintf("%s|PROTO_END|%s|duration=%.1fmin\n", format(proto_end, "%Y-%m-%d %H:%M:%OS3"),
            scaling_method, as.numeric(difftime(proto_end, proto_start, units = "mins"))),
    file = log_file, append = TRUE
  )
}

#1.2.5 Extract total within‐cluster sum of squares (WSS) for elbow plot  
wss_vec <- map_dbl(kp_list, "tot.withinss")
names(wss_vec) <- k_vals

#1.2.6 Log WSS for each k
for (j in seq_along(k_vals)) {
  cat(
    sprintf("%s|WSS|%s|k=%d|wss=%.1f\n", format(Sys.time(), "%Y-%m-%d %H:%M:%OS3"),
            scaling_method,  k_vals[j], wss_vec[j]),
    file = log_file, append = TRUE
  )
}

#1.3 *Now* read in all five validation objects and proceed to elbow & summary
indices <- c("silhouette","cindex","gamma","ptbiserial","tau")
val_list <- purrr::map(indices, ~ readRDS(
  file.path(validation_dir, sprintf("val_%s_%s.rds", scaling_method, .x))
))

#1.4 Bind & summarize all index results for plotting
index_k_df <- purrr::map_dfr(
  val_list,
  function(vr) {
    ks  <- as.integer(names(vr$indices))
    vals<- as.numeric(vr$indices)
    tibble(index = vr$method, k = ks, value = vals)
  }
)

#2. Generate elbow diagnostics & Kneedle (UIK)  
#2.1 Prepare a data.frame for plotting
elbow_df <- tibble(
  k = as.integer(names(wss_vec)),
  wss = as.numeric(wss_vec)
)

#2.2 Create first basic elbow plot
#2.2.1 Generate basic plot
elbow_plot <- ggplot(elbow_df, aes(x = k, y = wss)) +
  geom_line(size = 1) +
  geom_point(size = 3) +
  labs(
    x = "Number of clusters (k)",
    y = "Total within‐cluster sum of squares",
    title = "Elbow plot: WSS vs k") +
  theme_minimal(base_size = 14)

#2.2.2 Print the basic elbow plot
print(elbow_plot)

#2.3 Find the “knee” via the Kneedle (UIK) algorithm
knee <- uik(elbow_df$k, elbow_df$wss)

#2.3.1 Log the UIK elbow into the detailed log
cat(
  sprintf(
    "%s|UIK_ELBOW|%s|k=%d\n", format(Sys.time(), "%Y-%m-%d %H:%M:%OS3"),
    scaling_method, knee),
  file = log_file, append = TRUE)

#2.4 Create a more detailed elbow plot with UIK annotation
#2.4.1 Generate the elbow plot with the UIK
elbow_plot_uik <- ggplot(elbow_df, aes(x = k, y = wss)) +
  geom_line(size = 1) +
  geom_point(size = 3) +
  geom_vline(
    xintercept = knee,
    linetype = "dashed",
    size = 0.8,
    color = "red") +
  annotate(
    "text",
    x = knee,
    y = max(elbow_df$wss) * 0.95,
    label = paste0("UIK elbow:\nk = ", knee),
    hjust = 0,
    size = 5,
    color = "red") +
  labs(
    x = "Number of clusters (k)",
    y = "Total WSS",
    title = "Elbow plot with Kneedle (UIK) selection") +
  theme_minimal(base_size = 14)

#2.4.2 Print the elbow plot with the UIK
print(elbow_plot_uik)

#3. Summarize & visualize validation indices & pick a consensus k 
#3.1 Define which direction is “better” for each index
dir_tbl <- tribble(
  ~index, ~direction, ~label,
  "silhouette", "higher", "Silhouette (higher = better)",
  "cindex", "lower", "C-index (lower = better)",
  "gamma", "higher", "Gamma (higher = better)",
  "ptbiserial", "higher", "Point-biserial (higher = better)",
  "tau", "higher", "Tau (higher = better)"
)

#3.2 Pull out full index‐vs‐k curves from cached validation objects
index_k_df <- purrr::map_dfr(
  indices,
  function(idx) {
    vr_all <- readRDS(file.path(validation_dir, sprintf("val_%s_%s.rds", scaling_method, idx)))
    ks <- as.integer(names(vr_all$indices))
    vals <- as.numeric(vr_all$indices)
    tibble(index = idx, k = ks, value = vals)
  }
  ) %>%
  left_join(dir_tbl, by = "index")

#3.3 Compute each index’s best k
summary_tbl <- index_k_df %>%
  group_by(index, direction, label) %>%
  summarise(
    k_opt = if (direction[1] == "lower") k[which.min(value)] else k[which.max(value)],
    index_opt = if (direction[1] == "lower") min(value)       else max(value),
    .groups = "drop"
  )

#3.3.1 Determine “consensus” k (at least according to simple vote)
consensus_k <- summary_tbl$k_opt %>% table() %>% which.max() %>% names() %>% as.integer()

#3.3.2 Print and log the "consensus k"
cat(sprintf("%s|CONSENSUS_K|%s|%d\n", format(Sys.time(), "%Y-%m-%d %H:%M:%OS3"),
            scaling_method, consensus_k),
    file = log_file, append = TRUE)

#3.4 Create a line plot of all indices vs k
#3.4.1 Generate the line plot for each index x k value
index_line_plot <- ggplot(index_k_df, aes(x = k, y = value, color = label)) +
  geom_line(size = 1) +
  geom_point(size = 2) +
  geom_vline(xintercept = consensus_k, linetype = "dashed", color = "grey50") +
  annotate(
    "text", x = consensus_k + 0.2,
    y = max(index_k_df$value)*0.95,
    label = paste0("Consensus k = ", consensus_k),
    hjust = 0, size = 5) +
  labs(
    x = "Number of clusters (k)",
    y = "Validation-index value",
    color = "Index & direction",
    title = glue::glue("Validation-index trajectories ({scaling_method} scaling)")) +
  theme_minimal(base_size = 14)

#3.4.2 Print the line plot for each index x k value
print(index_line_plot)

#3.5 Create a heatmap of index values x each k
#3.5.1 Generate the heatmap
index_heatmap <- ggplot(index_k_df, aes(x = factor(k), y = label, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_viridis_c(option = "magma") +
  labs(
    x = "k",
    y = NULL,
    fill = "Value",
    title = "Heatmap of validation-index values") +
  theme_minimal(base_size = 14) +
  theme(axis.text.x = element_text(vjust = 0.5))

#3.5.2 Print the heatmap
print(index_heatmap)

#3.6 List the tabular summary of each index's optimal k & value
summary_tbl %>%
  arrange(label) %>%
  dplyr::select(label, k_opt, index_opt) %>%
  knitr::kable(
    caption = glue::glue("Optimal k per index ({scaling_method} scaling)"),
    col.names = c("Index (direction)", "optimal k", "Value"),
    digits = 3) %>%
  kableExtra::kable_styling(bootstrap_options = c("striped","hover","condensed","responsive"))

```

**Interpretation:**

- The elbow plot suggests a bend at *k* = `r print(as.numeric(knee))`, reinforced by Kneedle’s quantitative estimate

- Among the `r nrow(summary_tbl)` validation indices, `r sum(summary_tbl$k_opt == consensus_k)` (`r paste(summary_tbl$label[summary_tbl$k_opt == consensus_k], collapse = ", ")`) also vote for *k* = `r consensus_k`, while the others (`r paste0(summary_tbl$label[summary_tbl$k_opt != consensus_k], " (k=", summary_tbl$k_opt[summary_tbl$k_opt != consensus_k], ")", collapse = ", ")`) peak at neighboring values 

- Taken together, these results indicate that *k* = `r consensus_k` balances cohesion and separation most consistently in our risk variable data
